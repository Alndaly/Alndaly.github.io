"use strict";(self.webpackChunkclassic=self.webpackChunkclassic||[]).push([[4196],{61639:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>l,metadata:()=>r,toc:()=>s});var n=t(87462),i=(t(67294),t(3905));t(16758);const l={title:"yolov8\u7ed3\u5408opencv\u5b9e\u65f6\u68c0\u6d4b\u7269\u4f53\u5e76\u4e14\u505a\u51fa\u81ea\u5b9a\u4e49\u54cd\u5e94"},a=void 0,r={unversionedId:"artificial-intelligence/yolov8/yovov8\u7ed3\u5408opencv",id:"artificial-intelligence/yolov8/yovov8\u7ed3\u5408opencv",title:"yolov8\u7ed3\u5408opencv\u5b9e\u65f6\u68c0\u6d4b\u7269\u4f53\u5e76\u4e14\u505a\u51fa\u81ea\u5b9a\u4e49\u54cd\u5e94",description:"",source:"@site/docs/artificial-intelligence/yolov8/yovov8\u7ed3\u5408opencv.md",sourceDirName:"artificial-intelligence/yolov8",slug:"/artificial-intelligence/yolov8/yovov8\u7ed3\u5408opencv",permalink:"/docs/artificial-intelligence/yolov8/yovov8\u7ed3\u5408opencv",draft:!1,editUrl:"https://github.com/alndaly/alndaly.github.io/edit/master/docs/artificial-intelligence/yolov8/yovov8\u7ed3\u5408opencv.md",tags:[],version:"current",lastUpdatedAt:1693833086,formattedLastUpdatedAt:"2023\u5e749\u67084\u65e5",frontMatter:{title:"yolov8\u7ed3\u5408opencv\u5b9e\u65f6\u68c0\u6d4b\u7269\u4f53\u5e76\u4e14\u505a\u51fa\u81ea\u5b9a\u4e49\u54cd\u5e94"},sidebar:"ai",previous:{title:"mac\u4f7f\u7528GPU\u7684\u4e00\u4e9b\u95ee\u9898",permalink:"/docs/artificial-intelligence/tensorflow/mac\u4f7f\u7528GPU\u7684\u4e00\u4e9b\u95ee\u9898"},next:{title:"yolov8\u81ea\u5b9a\u4e49\u8bc6\u522b\u7269\u4f53",permalink:"/docs/artificial-intelligence/yolov8/\u81ea\u5b9a\u4e49\u8bc6\u522b\u7269\u4f53"}},c={},s=[],m={toc:s};function d(e){let{components:o,...t}=e;return(0,i.kt)("wrapper",(0,n.Z)({},m,t,{components:o,mdxType:"MDXLayout"}),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import cv2\nimport torch\nfrom ultralytics import YOLO\nfrom ultralytics.yolo.utils.plotting import Annotator\nfrom transformers import pipeline\n\n# Create a new YOLO model from scratch\nmodel = YOLO('yolov8n.yaml')\n\n# Load a pretrained YOLO model (recommended for training)\nmodel = YOLO('yolov8n.pt')\n\n# Train the model using the 'coco128.yaml' dataset for 3 epochs\nmodel.train(data='coco128.yaml', epochs=3)\n\ncapture = cv2.VideoCapture(0)\n\ndef do_something():\n    ...\n\nwhile(True):\n    # \u83b7\u53d6\u4e00\u5e27\n    ret, image = capture.read()\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    annotator = Annotator(image)\n    results = model(image, stream=False, verbose=False)\n    for item in results:\n        person_code = 0.0\n        # \u5224\u65ad\u662f\u5426\u5b58\u5728\u4eba\n        if(person_code in item.boxes.cls.tolist()):\n            print(\"\u68c0\u6d4b\u5230\u4eba\u51fa\u73b0\")\n            print(\"\u4f4d\u7f6e: {}\".format(item.boxes.boxes[item.boxes.cls.tolist().index(person_code)]))\n            print(\"\u53ef\u4fe1\u5ea6: {}\".format(item.boxes.conf.tolist()[item.boxes.cls.tolist().index(person_code)]))\n            annotator.box_label(item.boxes.xyxy[item.boxes.cls.tolist().index(person_code)], f\"person {item.boxes.conf.tolist()[item.boxes.cls.tolist().index(person_code)]}\")\n            # \u6b64\u5904\u53ef\u4ee5\u589e\u52a0\u81ea\u5b9a\u4e49\u64cd\u4f5c\n            do_something()\n\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    cv2.imshow('frame', image)\n    if cv2.waitKey(1) == ord('q'):\n        break\n")))}d.isMDXComponent=!0}}]);